{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipología y Ciclo de Vida de los Datos\n",
    "## Práctica 1: *Web scraping*\n",
    "### Javier Gómez de Diego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import builtwith\n",
    "import whois\n",
    "import time\n",
    "import warnings\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from chromedriver_py import binary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()                                               # Open the browser\n",
    "driver.get('https://about.netflix.com/es_es/new-to-watch')                # Load the page\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")   # Scroll down for the page to load completely\n",
    "time.sleep(1)                                                             # Wait for the rest of the page to load\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")   # Scroll to the bottom\n",
    "\n",
    "s = []                                                                    # Array for pages' content: every element is the soup for one of the pages\n",
    "s.append(BeautifulSoup(driver.page_source))                               # Save content of current page\n",
    "\n",
    "# Navigation throught all pages while saving the content\n",
    "pag = 0    \n",
    "sig = 2\n",
    "while len(driver.find_elements(By.XPATH, \"//*[text()='{sig:}']\".format(sig=sig))) != 0:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, \"//*[text()='{sig:}']\".format(sig=sig)).find_element(By.XPATH, \"./..\").click()   # Click button to go to the next page\n",
    "    time.sleep(3)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "    s.append(BeautifulSoup(driver.page_source))                                                                    # Save content\n",
    "    pag = pag + 1\n",
    "    sig = sig + 1\n",
    "         \n",
    "driver.close()                                                            # Close the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos = []                                                                 # Array for title name\n",
    "estreno = []                                                                 # Array for release date\n",
    "enlaces = []                                                                 # Array for movie's Netflix link\n",
    "portadas = []                                                                # Array for movie's cover art link\n",
    "\n",
    "# Go throught all pages saving the data\n",
    "j = 0\n",
    "for ss in s:\n",
    "    for a in ss.find_all('a'):\n",
    "        if (a.get('class') == ['link__StyledAnchor-sc-1gds0w3-0', 'cGrGmh']) & (a.p != None):\n",
    "            titulos.append(a.p.string)                                                                         # Save title name\n",
    "            for p in a.find_all('p'):\n",
    "                if p.get('class') == ['release-schedule-grid-cardstyles__DateText-sc-15f8nyv-2', 'dJFmAg']:\n",
    "                    estreno.append(p.string)                                                                   # Save release date\n",
    "                    enlaces.append(a.get('href'))                                                              # Save link\n",
    "    for i in ss.find_all('img'):\n",
    "        if j<len(titulos):\n",
    "            if i.get('alt') == titulos[j]:\n",
    "                portadas.append(i.get('src'))                                                                  # Save cover\n",
    "                j=j+1\n",
    "        else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Página 33 de 74 accedida correctamente.\r"
     ]
    }
   ],
   "source": [
    "# Set headers for multiple requests\n",
    "headers = {\n",
    "    'authority': 'scrapeme.live',\n",
    "    \"Accept-Language\": \"es-SP\",\n",
    "    'dnt': '1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/5\\\n",
    "    37.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36',\n",
    "}\n",
    "\n",
    "year = []                                                           # Array for year of production\n",
    "genero = []                                                         # Array for genre\n",
    "duracion = []                                                       # Array for duration (time or seassons)\n",
    "edad = []                                                           # Array for age rating\n",
    "protagonistas = []                                                  # Array for leading actors\n",
    "sinopsis = []                                                       # Array for synopsis\n",
    "\n",
    "# Access Netflix movies' sites\n",
    "for e, i in zip(enlaces, range(len(enlaces))):\n",
    "    time.sleep(.5)\n",
    "    page = requests.get(e, headers=headers)\n",
    "    \n",
    "    # Check response from server\n",
    "    if page.status_code >= 200 & page.status_code < 300:\n",
    "        print('Página ' + str(i) + ' de ' + str(len(enlaces) - 1) + ' accedida correctamente.', end='\\r')\n",
    "        \n",
    "        s = BeautifulSoup(page.content)\n",
    "        \n",
    "        # Extract genre\n",
    "        if s.find('a', {'class':'title-info-metadata-item item-genre'}) != None:\n",
    "            genero.append(s.find('a', {'class':'title-info-metadata-item item-genre'}).string)\n",
    "        else: genero.append('-')\n",
    "        \n",
    "        # Extract duration\n",
    "        if s.find('span', {'class':'duration'}) != None:\n",
    "            duracion.append(s.find('span', {'class':'duration'}).string)\n",
    "        elif s.find('span', {'class':'title-info-metadata-item item-runtime'}) != None:\n",
    "            duracion.append(s.find('span', {'class':'title-info-metadata-item item-runtime'}).string)\n",
    "        else: duracion.append('-')\n",
    "        \n",
    "        # Extract year\n",
    "        if s.find('span', {'class':'title-info-metadata-item item-year'}) != None:\n",
    "            year.append(s.find('span', {'class':'title-info-metadata-item item-year'}).string)\n",
    "        else: year.append('-')\n",
    "        \n",
    "        #Extract age rating\n",
    "        if s.find('span', {'class':'title-info-metadata-item item-maturity'}) != None:\n",
    "            edad.append(s.find('span', {'class':'title-info-metadata-item item-maturity'}).string)\n",
    "        else: edad.append('-')\n",
    "        \n",
    "        # Extract leading actors\n",
    "        if s.find('span', {'class':'title-data-info-item-list'}) != None:\n",
    "            protagonistas.append(s.find('span', {'class':'title-data-info-item-list'}).string)\n",
    "        else: protagonistas.append('-')\n",
    "        \n",
    "        # Extract synopsis\n",
    "        if s.find('div', {'class':'title-info-synopsis'}) != None:\n",
    "            sinopsis.append(s.find('div', {'class':'title-info-synopsis'}).string)\n",
    "        else: sinopsis.append('-')\n",
    "    \n",
    "    elif page.status_code == 403:\n",
    "        print('\\nPágina ' + str(i) + ' bloqueada.')\n",
    "    \n",
    "    else: print('\\nPágina ' + str(i) + ' no accedida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download covers and set unique title\n",
    "for i, j, k in zip(portadas, titulos, range(len(titulos))):\n",
    "    urllib.request.urlretrieve(i, (\"Portadas/\" + (str(k+1) + \"_\" + ''.join(char for char in j if char.isalnum())) + \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "netflixcomingsoonDF = pd.DataFrame({'Titulo':titulos,\n",
    "                                    'Fecha Estreno':estreno,\n",
    "                                    'Genero':genero,\n",
    "                                    'Duracion':duracion,\n",
    "                                    'Año':year,\n",
    "                                    'Clasificacion Edad':edad,\n",
    "                                    'Protagonistas':protagonistas,\n",
    "                                    'Sinopsis':sinopsis,\n",
    "                                    'Link':enlaces,\n",
    "                                    'Portada':portadas})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as CSV\n",
    "netflixcomingsoonDF.to_csv('Datos/NetflixComingSoon.csv', index=False, sep=';')  # Save as CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
